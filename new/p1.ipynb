{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (2.4.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (4.44.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.23.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (3.6.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (0.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (4.65.0)\n",
      "Requirement already satisfied: nlpaug in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.1.11)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (4.37.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from nlpaug) (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers scikit-learn pandas numpy matplotlib seaborn tqdm nlpaug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (1.23.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Requirement already satisfied: sympy in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2023.5.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aditya mohan khade\\appdata\\roaming\\python\\python310\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
      "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
      "mean   12681.192027      3.243473      0.280515            2.413711   \n",
      "std     7299.553863      0.883060      0.631851            1.399459   \n",
      "min        0.000000      3.000000      0.000000            0.000000   \n",
      "25%     6372.500000      3.000000      0.000000            2.000000   \n",
      "50%    12703.000000      3.000000      0.000000            3.000000   \n",
      "75%    18995.500000      3.000000      0.000000            3.000000   \n",
      "max    25296.000000      9.000000      7.000000            9.000000   \n",
      "\n",
      "            neither         class  \n",
      "count  24783.000000  24783.000000  \n",
      "mean       0.549247      1.110277  \n",
      "std        1.113299      0.462089  \n",
      "min        0.000000      0.000000  \n",
      "25%        0.000000      1.000000  \n",
      "50%        0.000000      1.000000  \n",
      "75%        0.000000      1.000000  \n",
      "max        9.000000      2.000000  \n",
      "Unnamed: 0            0\n",
      "count                 0\n",
      "hate_speech           0\n",
      "offensive_language    0\n",
      "neither               0\n",
      "class                 0\n",
      "tweet                 0\n",
      "dtype: int64\n",
      "0    23353\n",
      "1     1430\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils import resample\n",
    "import re\n",
    "import string\n",
    "from nlpaug.augmenter.word import SynonymAug\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data_path = '../Data/labeled_data.csv'\n",
    "df = pd.read_csv(data_path, encoding='latin1', delimiter=',', quotechar='\"')\n",
    "\n",
    "# Inspect the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['tweet'], inplace=True)\n",
    "\n",
    "# Map classes to binary labels\n",
    "df['label'] = df['class'].apply(lambda x: 1 if x == 0 else 0)  # 1: Hate Speech, 0: Non-Hate Speech\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[['tweet', 'label']]\n",
    "\n",
    "# Check class distribution\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove mentions (@user)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['clean_tweet'] = df['tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    23353\n",
      "1    23353\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df[df.label == 0]  # Non-Hate Speech\n",
    "df_minority = df[df.label == 1]  # Hate Speech\n",
    "\n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # Sample with replacement\n",
    "                                 n_samples=len(df_majority),    # Match majority class\n",
    "                                 random_state=42)  # Reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Verify balanced class distribution\n",
    "print(df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Mohan Khade\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize and encode data\n",
    "def encode_data(texts, tokenizer, max_length=128):\n",
    "    encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "    return encodings\n",
    "\n",
    "# Encode training and testing data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_balanced['clean_tweet'], df_balanced['label'], test_size=0.2, random_state=42, stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "train_encodings = encode_data(train_texts, tokenizer)\n",
    "val_encodings = encode_data(val_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HateSpeechDataset(train_encodings, train_labels)\n",
    "val_dataset = HateSpeechDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir='./results_bert',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer_bert = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer_bert.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results_bert = trainer_bert.evaluate()\n",
    "print(results_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "predictions_bert = trainer_bert.predict(val_dataset)\n",
    "pred_labels_bert = predictions_bert.predictions.argmax(-1)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(val_labels, pred_labels_bert, target_names=['Non-Hate Speech', 'Hate Speech']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout=0.5):\n",
    "        super(LSTM_CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=cnn_hidden_dim, kernel_size=3)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2 + cnn_hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        embedded_permuted = embedded.permute(0, 2, 1)  # Permute for CNN\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        conv_out = self.conv(embedded_permuted)\n",
    "        conv_out = F.relu(conv_out)\n",
    "        conv_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "        combined_out = torch.cat((lstm_out[:, -1, :], conv_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        logits = self.fc(combined_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "X_vectorized = vectorizer.fit_transform(df_balanced['clean_tweet']).toarray()\n",
    "X_train_lstm, X_val_lstm, y_train_lstm, y_val_lstm = train_test_split(\n",
    "    X_vectorized, df_balanced['label'], test_size=0.2, random_state=42, stratify=df_balanced['label']\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train_lstm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_lstm.values, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val_lstm, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_lstm.values, dtype=torch.long)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset_lstm = Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
    "val_dataset_lstm = Dataset.from_tensor_slices((X_val_tensor, y_val_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = 10000\n",
    "embed_dim = 128\n",
    "lstm_hidden_dim = 128\n",
    "cnn_hidden_dim = 128\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model\n",
    "model_lstm_cnn = LSTM_CNN(vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lstm_cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_lstm_cnn.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 8\n",
    "\n",
    "train_loader_lstm_cnn = DataLoader(train_dataset_lstm, batch_size=batch_size, shuffle=True)\n",
    "val_loader_lstm_cnn = DataLoader(val_dataset_lstm, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_lstm_cnn.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader_lstm_cnn:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_lstm_cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader_lstm_cnn)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model_lstm_cnn.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_lstm_cnn:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model_lstm_cnn(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predict on validation set\n",
    "model_lstm_cnn.eval()\n",
    "preds_list = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader_lstm_cnn:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model_lstm_cnn(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        preds_list.extend(preds.cpu().numpy())\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_val_lstm, preds_list, target_names=['Non-Hate Speech', 'Hate Speech']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize synonym augmentation\n",
    "aug = SynonymAug(aug_src='wordnet')\n",
    "\n",
    "# Augment text\n",
    "augmented_texts = [aug.augment(text) for text in train_texts]\n",
    "\n",
    "# Combine original and augmented texts\n",
    "train_texts_augmented = train_texts.tolist() + augmented_texts\n",
    "train_labels_augmented = train_labels.tolist() + train_labels.tolist()\n",
    "\n",
    "# Shuffle the augmented data\n",
    "augmented_df = pd.DataFrame({'clean_tweet': train_texts_augmented, 'label': train_labels_augmented})\n",
    "augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Encode augmented data\n",
    "train_encodings_augmented = encode_data(augmented_df['clean_tweet'], tokenizer)\n",
    "\n",
    "# Create augmented dataset\n",
    "train_dataset_augmented = HateSpeechDataset(train_encodings_augmented, augmented_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Define function for adversarial training\n",
    "def adversarial_training(model, inputs, labels, epsilon=0.01):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    outputs = model(**inputs, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    \n",
    "    # Get gradient of loss w.r.t. input embeddings\n",
    "    embed_grad = inputs['input_ids'].grad\n",
    "    \n",
    "    # Create adversarial perturbation\n",
    "    perturbed_inputs = inputs['input_ids'] + epsilon * embed_grad.sign()\n",
    "    \n",
    "    # Forward pass with perturbed inputs\n",
    "    adv_outputs = model(input_ids=perturbed_inputs, attention_mask=inputs['attention_mask'], labels=labels)\n",
    "    adv_loss = adv_outputs.loss\n",
    "    \n",
    "    # Backpropagate adversarial loss\n",
    "    adv_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Reinitialize model and optimizer\n",
    "model_adv = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "optimizer_adv = AdamW(model_adv.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop with adversarial training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_adv.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "batch_size = 8\n",
    "\n",
    "train_loader_adv = DataLoader(train_dataset_augmented, batch_size=batch_size, shuffle=True)\n",
    "val_loader_adv = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_adv.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader_adv:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        labels = inputs.pop('labels')\n",
    "        \n",
    "        optimizer_adv.zero_grad()\n",
    "        outputs = model_adv(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward(retain_graph=True)  # Retain graph for adversarial training\n",
    "        \n",
    "        adversarial_training(model_adv, inputs, labels)\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader_adv)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model_adv.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader_adv:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            labels = inputs.pop('labels')\n",
    "            \n",
    "            outputs = model_adv(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on validation set\n",
    "model_adv.eval()\n",
    "preds_list_adv = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader_adv:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        labels = inputs.pop('labels')\n",
    "        \n",
    "        outputs = model_adv(**inputs)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "        preds_list_adv.extend(preds.cpu().numpy())\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(val_labels, preds_list_adv, target_names=['Non-Hate Speech', 'Hate Speech']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting classification reports\n",
    "def plot_classification_report(report, title):\n",
    "    report_df = pd.DataFrame(report).iloc[:-1, :].T\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(report_df, annot=True, fmt=\".2f\", cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Classification report for BERT\n",
    "report_bert = classification_report(val_labels, pred_labels_bert, target_names=['Non-Hate Speech', 'Hate Speech'], output_dict=True)\n",
    "plot_classification_report(report_bert, \"BERT Model Classification Report\")\n",
    "\n",
    "# Classification report for LSTM + CNN\n",
    "report_lstm_cnn = classification_report(y_val_lstm, preds_list, target_names=['Non-Hate Speech', 'Hate Speech'], output_dict=True)\n",
    "plot_classification_report(report_lstm_cnn, \"LSTM + CNN Model Classification Report\")\n",
    "\n",
    "# Classification report for Adversarial Training\n",
    "report_adv = classification_report(val_labels, preds_list_adv, target_names=['Non-Hate Speech', 'Hate Speech'], output_dict=True)\n",
    "plot_classification_report(report_adv, \"Adversarial Training Model Classification Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the BERT model\n",
    "model_bert.save_pretrained('./hate_speech_model_bert')\n",
    "tokenizer.save_pretrained('./hate_speech_model_bert')\n",
    "\n",
    "# Save the LSTM + CNN model\n",
    "torch.save(model_lstm_cnn.state_dict(), './hate_speech_model_lstm_cnn.pth')\n",
    "\n",
    "# Save the Adversarial Training model\n",
    "model_adv.save_pretrained('./hate_speech_model_adv')\n",
    "tokenizer.save_pretrained('./hate_speech_model_adv')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import pipeline\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained BERT model\n",
    "classifier_bert = pipeline('text-classification', model='./hate_speech_model_bert')\n",
    "\n",
    "@app.route('/predict_bert', methods=['POST'])\n",
    "def predict_bert():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    result = classifier_bert(text)\n",
    "    return jsonify(result)\n",
    "\n",
    "# Load the trained LSTM + CNN model\n",
    "class LSTM_CNN_Deploy(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout=0.5):\n",
    "        super(LSTM_CNN_Deploy, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, lstm_hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=cnn_hidden_dim, kernel_size=3)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim * 2 + cnn_hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        embedded_permuted = embedded.permute(0, 2, 1)  # Permute for CNN\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        conv_out = self.conv(embedded_permuted)\n",
    "        conv_out = F.relu(conv_out)\n",
    "        conv_out = F.max_pool1d(conv_out, conv_out.size(2)).squeeze(2)\n",
    "        combined_out = torch.cat((lstm_out[:, -1, :], conv_out), dim=1)\n",
    "        combined_out = self.dropout(combined_out)\n",
    "        logits = self.fc(combined_out)\n",
    "        return logits\n",
    "\n",
    "# Initialize deployed LSTM + CNN model\n",
    "model_deploy = LSTM_CNN_Deploy(vocab_size, embed_dim, lstm_hidden_dim, cnn_hidden_dim, num_classes, dropout)\n",
    "model_deploy.load_state_dict(torch.load('./hate_speech_model_lstm_cnn.pth'))\n",
    "model_deploy.to(device)\n",
    "model_deploy.eval()\n",
    "\n",
    "# Function to preprocess and encode text for LSTM + CNN\n",
    "def preprocess_and_encode_lstm_cnn(text, vectorizer, tokenizer, max_length=128):\n",
    "    text = preprocess_text(text)\n",
    "    vectorized_text = vectorizer.transform([text]).toarray()\n",
    "    vectorized_tensor = torch.tensor(vectorized_text, dtype=torch.float32).to(device)\n",
    "    return vectorized_tensor\n",
    "\n",
    "@app.route('/predict_lstm_cnn', methods=['POST'])\n",
    "def predict_lstm_cnn():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    inputs = preprocess_and_encode_lstm_cnn(text, vectorizer, tokenizer)\n",
    "    outputs = model_deploy(inputs)\n",
    "    pred_label = torch.argmax(outputs, dim=1).item()\n",
    "    result = [{'label': ['Non-Hate Speech', 'Hate Speech'][pred_label], 'score': F.softmax(outputs, dim=1)[0][pred_label].item()}]\n",
    "    return jsonify(result)\n",
    "\n",
    "# Load the trained Adversarial Training model\n",
    "classifier_adv = pipeline('text-classification', model='./hate_speech_model_adv')\n",
    "\n",
    "@app.route('/predict_adv', methods=['POST'])\n",
    "def predict_adv():\n",
    "    data = request.json\n",
    "    text = data['text']\n",
    "    result = classifier_adv(text)\n",
    "    return jsonify(result)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Sample text\n",
    "sample_text = {\"text\": \"I hate that bitch\"}\n",
    "\n",
    "# Test BERT model\n",
    "response_bert = requests.post('http://127.0.0.1:5000/predict_bert', json=sample_text)\n",
    "print(response_bert.json())\n",
    "\n",
    "# Test LSTM + CNN model\n",
    "response_lstm_cnn = requests.post('http://127.0.0.1:5000/predict_lstm_cnn', json=sample_text)\n",
    "print(response_lstm_cnn.json())\n",
    "\n",
    "# Test Adversarial Training model\n",
    "response_adv = requests.post('http://127.0.0.1:5000/predict_adv', json=sample_text)\n",
    "print(response_adv.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
